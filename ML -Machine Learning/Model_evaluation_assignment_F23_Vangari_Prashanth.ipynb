{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9M61rPmeVanA",
      "metadata": {
        "id": "9M61rPmeVanA"
      },
      "source": [
        "## Student name: Vangari Prashanth\n",
        "## Student Id: 11645119\n",
        "## Assignment: Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2R1vOn9oVanE",
      "metadata": {
        "id": "2R1vOn9oVanE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Read the data stored in your local machine https://www.kaggle.com/datasets/andrewmvd/fetal-health-classification\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('fetal_health.csv')\n",
        "X = df.drop(columns='fetal_health')\n",
        "y = df['fetal_health']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lnWHbkbCVanF",
      "metadata": {
        "id": "lnWHbkbCVanF"
      },
      "source": [
        "### Part 1: Perform classification task using 5 different models (75 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wAanjyZlVanF",
      "metadata": {
        "id": "wAanjyZlVanF"
      },
      "source": [
        "###### For each model (except Naive Bayes), use GridSearchCV() to tune the hyperparameters (note: testing ranges are specified in the assignment description)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wu4yxsJdVanF",
      "metadata": {
        "id": "Wu4yxsJdVanF"
      },
      "source": [
        "###### Logistic Regression with L1 penalty (Lasso) (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "AlydZvxnVanF",
      "metadata": {
        "id": "AlydZvxnVanF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "UDn65LZcVanG",
      "metadata": {
        "id": "UDn65LZcVanG"
      },
      "source": [
        "###### Decision Tree (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3Irq4hAdVanG",
      "metadata": {
        "id": "3Irq4hAdVanG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bLpaiUVyVanG",
      "metadata": {
        "id": "bLpaiUVyVanG"
      },
      "source": [
        "###### KNN (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "RYVAJ2hEVanG",
      "metadata": {
        "id": "RYVAJ2hEVanG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Sa6-BiO2VanG",
      "metadata": {
        "id": "Sa6-BiO2VanG"
      },
      "source": [
        "###### SVC (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "nTzvP4YjVanG",
      "metadata": {
        "id": "nTzvP4YjVanG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "_BXly7uIVanG",
      "metadata": {
        "id": "_BXly7uIVanG"
      },
      "source": [
        "###### SGD (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "vg6qOAoNVanH",
      "metadata": {
        "id": "vg6qOAoNVanH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fHYydYDgVanH",
      "metadata": {
        "id": "fHYydYDgVanH"
      },
      "source": [
        "### Part 2: Compare 5 different models' accuracies (25 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nN8jj72zVanH",
      "metadata": {
        "id": "nN8jj72zVanH"
      },
      "source": [
        "#### Use the best hyperparameters returned from GridSearchCV to re-train the models, and compare the accuracies of all 5 models.\n",
        "The followings include hyperparameters of mentioned models:\n",
        "* Logistic Regression:\n",
        "    * C: from 0.1 to 1, step = 0.3\n",
        "    * multi_class: auto, ovr, multinomial\n",
        "    * solver: newton-cg\n",
        "* Decision Tree:\n",
        "    * criterion: gini, entropy, log_loss\n",
        "    * max_features: sqrt, log2\n",
        "    * max_depth: 2 to 5\n",
        "* KNN:\n",
        "    * n_neighbors: 3 to 7\n",
        "    * weights: uniform, distance\n",
        "* SVC:\n",
        "    * degree: 2 to 5\n",
        "    * C: 0.1 to 1, step = 0.3\n",
        "    * kernel: poly, rbf\n",
        "* SGD:\n",
        "    * loss: hinge, log_loss, modified_huber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2u1-o3jfeXkh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u1-o3jfeXkh",
        "outputId": "26180b28-9c43-4377-f215-43a124d94521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best hyperparameters: {'C': 1.0, 'multi_class': 'ovr', 'solver': 'newton-cg'}\n",
            "Accuracy of best Logistic Regression: 0.8943661971830986\n"
          ]
        }
      ],
      "source": [
        "\n",
        "'''\n",
        "This code uses grid search cross-validation and hyperparameter tuning to optimize a logistic regression model.\n",
        "A hypergrid parameters is used . To find the ideal set of hyperparameters, such as regularization strength (C), multi-class strategy, and solver algorithm, GridSearchCV is utilized. Following a fitting of the model to the training set, the optimal hyperparameters are reported.\n",
        "These ideal hyperparameters are used to instantiate a new logistic regression model, which is then trained using the training set. The test data's target variable is predicted using the final model, and the model's accuracy is computed and reported, offering an evaluation of the logistic regression model's performance.\n",
        "'''\n",
        "\n",
        "#1. Logistic Regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "logRegModel = LogisticRegression()\n",
        "\n",
        "pg = {\n",
        "    'C': [0.1, 0.4, 0.7, 1.0],\n",
        "    'multi_class': ['auto', 'ovr', 'multinomial'],\n",
        "    'solver': ['newton-cg']\n",
        "}\n",
        "\n",
        "# make a GridSearchCV attribute example\n",
        "grd_srch = GridSearchCV(estimator=logRegModel, param_grid=pg, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grd_srch.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found by GridSearchCV\n",
        "print('best hyperparameters:', grd_srch.best_params_)\n",
        "\n",
        "\n",
        "# Create a new Logistic Regression model with the best hyperparameters\n",
        "fine_logRegModel = LogisticRegression(**grd_srch.best_params_)\n",
        "\n",
        "\n",
        "# Fit the Logistic Regression model with the best hyperparameters to the training data\n",
        "fine_logRegModel.fit(X_train, y_train)\n",
        "\n",
        "# Use the best model to predict the target variable for the test data\n",
        "y_pred = fine_logRegModel.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best Logistic Regression model on the test data\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy of best Logistic Regression:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1Xm7Cpg4eYRI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xm7Cpg4eYRI",
        "outputId": "b40e2f18-6735-4ffa-b4a0-2c7b26cf6252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best hyperparameters: {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt'}\n",
            "Accuracy of the best Decision Tree: 0.892018779342723\n"
          ]
        }
      ],
      "source": [
        "#2.Decision tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import warnings\n",
        "\n",
        "# Suppress DeprecationWarnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define a function to ignore DeprecationWarnings\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "# Use the function within a context manager to catch and ignore DeprecationWarnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "dec_tree = DecisionTreeClassifier()\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "pg = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'max_depth': range(2, 6)\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object with Decision Tree classifier, hyperparameter grid, and 5-fold cross-validation\n",
        "gd_sch_cv2 = GridSearchCV(dec_tree, pg, cv=5)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "gd_sch_cv2.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found by GridSearchCV\n",
        "print(\"best hyperparameters:\", gd_sch_cv2.best_params_)\n",
        "\n",
        "# Create a new Decision Tree model with the best hyperparameters\n",
        "fine_dt = DecisionTreeClassifier(criterion=gd_sch_cv2.best_params_['criterion'],\n",
        "                                 max_features=gd_sch_cv2.best_params_['max_features'],\n",
        "                                 max_depth=gd_sch_cv2.best_params_['max_depth'])\n",
        "# Fit the Decision Tree model with the best hyperparameters to the training data\n",
        "fine_dt.fit(X_train, y_train)\n",
        "\n",
        "# Use the best model to predict the target variable for the test data\n",
        "y_pred = fine_dt.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best Decision Tree model on the test data\n",
        "acc1 = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of the best Decision Tree:\", acc1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9ZZWvHE4eYqS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZZWvHE4eYqS",
        "outputId": "24ebc802-5461-41ea-ef16-6c84b09e276b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best hyperparameters:  {'n_neighbors': 3, 'weights': 'distance'}\n",
            "Accuracy of best KNN:  0.9084507042253521\n"
          ]
        }
      ],
      "source": [
        "#3 KNN\n",
        "\n",
        "'''\n",
        "Scikit-learn's `KNeighborsClassifier` is used in the machine learning workflow to implement the K-Nearest Neighbors (KNN) algorithm. Optimizing the classifier through hyperparameter tuning to improve model performance is an essential step.\n",
        "The method used to achieve this is a grid search over a predefined parameter grid with two weighting strategies ('distance' and 'uniform') and various values for the number of neighbors. To find the best configurations, the search, assisted by {GridSearchCV}, runs a cross-validated assessment for every set of parameters. A new KNN classifier is instantiated with these best-found parameters and fitted to the training data after the optimal hyperparameters have been determined. A different test dataset is then used to assess the effectiveness of the model, and the accuracy metric offers a numerical representation of the classifier's predictive capability.\n",
        "'''\n",
        "\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Finding best Hyperparameters\n",
        "pg = {'n_neighbors': [3, 4, 5, 6, 7], 'weights': ['uniform', 'distance']}\n",
        "\n",
        "\n",
        "grd_srchcv3 = GridSearchCV(knn, pg, cv=5)\n",
        "\n",
        "\n",
        "grd_srchcv3.fit(X_train, y_train)\n",
        "\n",
        "# Deliver the best hyperparameters\n",
        "print(\"best hyperparameters: \", grd_srchcv3.best_params_)\n",
        "\n",
        "# to fit the model utilize the best hyperparameters\n",
        "fne_knn = KNeighborsClassifier(n_neighbors=grd_srchcv3.best_params_['n_neighbors'], weights=grd_srchcv3.best_params_['weights'])\n",
        "fne_knn.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = fne_knn.predict(X_test)\n",
        "\n",
        "# finding Accuracy\n",
        "acc2 = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of best KNN: \", acc2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "g6vQfLwBeZC1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6vQfLwBeZC1",
        "outputId": "cacecfd5-13b4-417d-d107-dcd5184f7762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best Hyperparameters: degree=5, C=0.7, kernel=poly\n",
            "Accuracy of best SVC:  0.8849765258215962\n"
          ]
        }
      ],
      "source": [
        "#4.SVC\n",
        "\n",
        "'''\n",
        "We describe here how to use the `SVC` module from Scikit-learn to fine-tune a Support Vector Classifier (SVC). The SVC model is very flexible, and its ideal configuration for a given dataset can be found by adjusting a number of hyperparameters. Using both polynomial and radial basis function (RBF) kernels, a grid search, carried out via `GridSearchCV}, investigates a range of values for the polynomial degree, the regularization parameter {C}, and the kernel type. The combinations of these parameters are cross-validated in this exhaustive search to determine the best model settings for the training data.\n",
        "After that, a new SVC instance is configured using the optimal parameters, and it is retrained using the training set.\n",
        "'''\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "svc = SVC()\n",
        "\n",
        "pm = {'degree': [2, 3, 4, 5], 'C': [0.1, 0.4, 0.7, 1.0], 'kernel': ['poly', 'rbf']}\n",
        "\n",
        "\n",
        "clf = GridSearchCV(svc, pm)\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "fin_deg = clf.best_params_['degree']\n",
        "f_clf = clf.best_params_['C']\n",
        "f_krnl = clf.best_params_['kernel']\n",
        "\n",
        "print(\"best Hyperparameters: degree={}, C={}, kernel={}\".format(fin_deg, f_clf, f_krnl))\n",
        "\n",
        "# create a new instance of the SVC model with the best hyperparameters\n",
        "f_svc_mdl = SVC(degree=fin_deg, C=f_clf, kernel=f_krnl)\n",
        "\n",
        "\n",
        "f_svc_mdl.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = f_svc_mdl.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "acc3 = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of best SVC: \", acc3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "m-KArH57eZoh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-KArH57eZoh",
        "outputId": "64750002-41ae-41bf-b2e1-55b4dcb4b666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best hyperparameters: {'loss': 'hinge'}\n",
            "Accuracy of best SGD: 0.8356807511737089\n"
          ]
        }
      ],
      "source": [
        "#5.SGD\n",
        "\n",
        "'''\n",
        "The Stochastic Gradient Descent (SGD) Classifier is a linear classifier that can handle large amounts of data effectively. The documentation explains how to use it. During the model optimization process, Scikit-learn's `GridSearchCV` is used to evaluate several loss functions, including 'hinge', 'log', and'modified_huber', in order to determine which one works best for the given dataset. This grid search is carried out in parallel to speed up computation and uses cross-validation to guarantee robustness against overfitting. Following the identification and reporting of the optimal hyperparameters, the classifier is redesigned using these parameters to create the final SGD model.\n",
        "Predictions on the test set are then made using this refined model.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "\n",
        "sgd = SGDClassifier()\n",
        "\n",
        "# Finding best Hyperparameters\n",
        "pms = {'loss': ['hinge', 'log', 'modified_huber']}\n",
        "\n",
        "# create an instance of GridSearchCV and fit it on the training data\n",
        "grd_srchcv5 = GridSearchCV(sgd, pms, cv=5, n_jobs=-1)\n",
        "grd_srchcv5.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print('best hyperparameters:', grd_srchcv5.best_params_)\n",
        "\n",
        "# predict the target variable using the best model\n",
        "f_sgd = grd_srchcv5.best_estimator_\n",
        "y_pred = f_sgd.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "acc4 = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy of best SGD:', acc4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "BfD9YzH0VanH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfD9YzH0VanH",
        "outputId": "b8c84596-8fa5-4675-96a4-36f2b5fca402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Logistic Regression accuracy: 0.8943661971830986\n",
            "Best Decision Tree accuracy: 0.8779342723004695\n",
            "Best KNN accuracy:  0.9084507042253521\n",
            "Best SVC Accuracy:  0.8849765258215962\n",
            "Best SGD accuracy: 0.8356807511737089\n"
          ]
        }
      ],
      "source": [
        "# Best Hyperparameters for Logistic Regression\n",
        "\n",
        "'''\n",
        "With hyperparameter optimization, the Logistic Regression model shows improved predictive power on the test set. As a measure of the model's performance and potential for generalization, the accuracy score quantifies the model's predictions.\n",
        "\n",
        "The maximum features, depth parameters, and best-found criterion are specifically tailored into the Decision Tree Classifier. The predictive accuracy of the model is calculated after retraining with the optimized hyperparameters, offering information about how well the model performs the classification task.\n",
        "\n",
        "The number of neighbors and weighting scheme for the K-Nearest Neighbors (KNN) classifier are calibrated for best results. The accuracy of the trained model is then evaluated, providing insight into how well the KNN approach works in the particular situation.\n",
        "\n",
        "The kernel type, regularization parameter, and ideal degree of the Support Vector Classifier (SVC) are set up. The SVC's performance in high-dimensional spaces is demonstrated when the model is subsequently trained and its accuracy is assessed.\n",
        "\n",
        "Finally, the best estimator is used to implement the Stochastic Gradient Descent (SGD) Classifier. The model's ability to perform linear classification, particularly on large datasets, is demonstrated by the accuracy of its predictions.\n",
        "'''\n",
        "\n",
        "\n",
        "fine_logRegModel = LogisticRegression(**grd_srch.best_params_)\n",
        "\n",
        "\n",
        "fine_logRegModel.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = fine_logRegModel.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Best Logistic Regression accuracy:', accuracy)\n",
        "\n",
        "# Decision Tree with best hyperparameters\n",
        "dt_best = DecisionTreeClassifier(criterion=gd_sch_cv2.best_params_['criterion'],\n",
        "                                 max_features=gd_sch_cv2.best_params_['max_features'],\n",
        "                                 max_depth=gd_sch_cv2.best_params_['max_depth'])\n",
        "dt_best.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = dt_best.predict(X_test)\n",
        "\n",
        "acc1 = accuracy_score(y_test, y_pred)\n",
        "# Print accuracy for Decision Tree\n",
        "print(\"Best Decision Tree accuracy:\", acc1)\n",
        "\n",
        "# K-Nearest Neighbors with best hyperparameters\n",
        "fne_knn = KNeighborsClassifier(n_neighbors=grd_srchcv3.best_params_['n_neighbors'], weights=grd_srchcv3.best_params_['weights'])\n",
        "fne_knn.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = fne_knn.predict(X_test)\n",
        "\n",
        "\n",
        "acc2 = accuracy_score(y_test, y_pred)\n",
        "# Print accuracy for KNN\n",
        "print(\"Best KNN accuracy: \", acc2)\n",
        "\n",
        "\n",
        "\n",
        "# SVC with best Hyperparameters\n",
        "\n",
        "# create a new instance of the SVC model with the best hyperparameters\n",
        "f_svc_mdl = SVC(degree=fin_deg, C=f_clf, kernel=f_krnl)\n",
        "\n",
        "# fit the f_svc_mdl on the training data\n",
        "f_svc_mdl.fit(X_train, y_train)\n",
        "\n",
        "# Using the best hyperparameters, a decision tree\n",
        "y_pred = f_svc_mdl.predict(X_test)\n",
        "\n",
        "# calculate the accuracy score for f_svc_mdl\n",
        "acc3 = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# print accuracy\n",
        "print(\"Best SVC Accuracy: \", acc3)\n",
        "\n",
        "\n",
        "#Predict the target variable for the test data using the best model.\n",
        "f_sgd = grd_srchcv5.best_estimator_\n",
        "y_pred = f_sgd.predict(X_test)\n",
        "\n",
        "# Accuracy score is calculated\n",
        "acc4 = accuracy_score(y_test, y_pred)\n",
        "print('Best SGD accuracy:', acc4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nmkA93CQvI4-",
      "metadata": {
        "id": "nmkA93CQvI4-"
      },
      "source": [
        "**Observation**:\n",
        "Based on the above, accuracy score, we can conclude that KNN has the highest accuracy score with 0.9084507042253521 where as SGD has least accuracy score with 0.8356807511737089"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}